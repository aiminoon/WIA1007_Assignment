{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "University of Malaya\n",
        "\n",
        "Faculty of Computer Science & Information Technology\n",
        "\n",
        "WIA1007 Introduction to Data Science\n",
        "\n",
        "Group Assignment (Predictive Maintenance, SDGs related)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "p0Y6Eq0Gd8nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "Group 17\n",
        "Dennis Aimin Oon bin Jeffrey Oon - 22001610\n",
        "Adib Rusyaidi Bin Mohd Zaki - 22001821\n",
        "Ahmad Firdaus Bin Ahmad Hafiz - 22002552\n",
        "Amirul Farhan Bin Amir Hamzah - 22002786\n",
        "Muhammad Imran bin Ilias - 22001723\n",
        "```\n"
      ],
      "metadata": {
        "id": "Aubxrys9c_8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preprocessing\n"
      ],
      "metadata": {
        "id": "6xKIKNMtAcm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Data Loading\n"
      ],
      "metadata": {
        "id": "XOq0URvIAjst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0lJURgGSji8",
        "outputId": "633ff400-c33f-4eed-c586-4ab6f907a9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'credit_risk_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7da6d70ade76>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the csv dataset into a 'dataframe' df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'credit_risk_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# https://drive.google.com/file/d/1QnUVuycelO5XlXQGgwJomaVNXwLALAyO/view?usp=sharing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# The dataset is about Credit Risk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'credit_risk_dataset.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the csv dataset into a 'dataframe' df\n",
        "df = pd.read_csv('credit_risk_dataset.csv')\n",
        "# You can download the credit_risk_dataset.csv file under \"data\" folder.\n",
        "# The dataset is about Credit Risk."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is loaded into the dataframe from the project directory using pandas.\n",
        "\n",
        "The dataframe is given the name df."
      ],
      "metadata": {
        "id": "HAvM8D35MQwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1.2 Initial Data Exploration\n"
      ],
      "metadata": {
        "id": "4JVVHanyAyQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 rows of the dataset to get an initial sense of the data.\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "aQ-8IyjKSwIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows data types and non-null values for every columns.\n",
        "df.info()"
      ],
      "metadata": {
        "id": "iOvplb3aSy6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From df.info(), we knew the total features and samples we have.\n",
        "* The dataset has 32581 samples, or rows of observation with 12 columns.\n",
        "* Have 12 features determining 1 target output (loan status)\n",
        "* 12 features is good enough\n",
        "* Have 32581 samples\n",
        "* Loan status data has enough samples"
      ],
      "metadata": {
        "id": "keAzdChiIIHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From df.info(), we knew the features data type.\n",
        "\n",
        "int64\n",
        "* person_age\n",
        "* loan_amnt\n",
        "* loan_status\n",
        "* cb_person_cred_hist_length\n",
        "\n",
        "float64\n",
        "* person_emp_length\n",
        "* loan_int_rate\n",
        "* loan_percent_income\n",
        "\n",
        "object\n",
        "* person_home_ownership\n",
        "* loan_intent\n",
        "* loan_grade\n",
        "* cb_person_default_on_file"
      ],
      "metadata": {
        "id": "M65Yg_FBIW6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows the total number of missing values per column.\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "WRIAq3H5S-mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From df.isna().sum(), we know the amount of null values in each column in the dataframe.\n",
        "\n",
        "\n",
        "*   person_emp_length column have 895 total null values\n",
        "*   loan_int_rate column have 3116 total null values\n",
        "*   All other columns have no null values\n",
        "\n"
      ],
      "metadata": {
        "id": "4OZKjq07Mjxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows the missing values in Boolean.\n",
        "df.isna()"
      ],
      "metadata": {
        "id": "-lHnecVATBA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.isna() shows whether there are missing values or not in each column in a row.\n",
        "\n",
        "*   False means that there are no missing values in the column in the row.\n",
        "\n",
        "*   True means there are missing values in the column in the row."
      ],
      "metadata": {
        "id": "X2K-omJ3NoxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows summary statistics (e.g., min, max, mean, std deviation, and quartiles).\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "NBjL1JY3TDSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.describe() returns the description of the data in each column in the dataframe.\n",
        "*   count gives the total amount of data in each columns\n",
        "*   mean gives the average value of the whole column for each columns\n",
        "*   std gives the standard deviation of each value in the column for each columns\n",
        "*   min gives the lowest value in each columns\n",
        "*   25% gives the number of values lower than 25% percentile of the column\n",
        "*   50% gives the number of values lower than 50% percentile of the column\n",
        "*   75% gives the number of values lower than 75% percentile of the column\n",
        "*   max gives the highest value in each columns"
      ],
      "metadata": {
        "id": "lrJrlSC0OUor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Data Cleaning"
      ],
      "metadata": {
        "id": "BHX6_fqsSJuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.3.1 Changes to the dataframe to make it understandable"
      ],
      "metadata": {
        "id": "koY3vhEZT1Ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the column from 'person_age' to 'age'\n",
        "df.rename(columns={'person_age' : 'age'}, inplace=True)\n",
        "\n",
        "# Rename the column from 'person_income' to 'income'\n",
        "df.rename(columns={'person_income': 'income'},inplace=True)\n",
        "\n",
        "# Rename the column from 'person_home_ownership' to 'home_ownership'\n",
        "df.rename(columns={'person_home_ownership': 'home_ownership'},inplace=True)\n",
        "\n",
        "# Rename the column from 'person_emp_length' to 'employment_years'\n",
        "df.rename(columns={'person_emp_length': 'employment_years'},inplace=True)\n",
        "\n",
        "# Rename the column from 'loan_amnt' to 'loan_amount'\n",
        "df.rename(columns={'loan_amnt': 'loan_amount'},inplace=True)\n",
        "\n",
        "# Rename the column from 'loan_percent_income' to 'loan_per_income'\n",
        "df.rename(columns={'loan_percent_income': 'loan_per_income'},inplace=True)\n",
        "\n",
        "# Rename the column from 'cb_person_default_on_file' to 'historical_default'\n",
        "df.rename(columns={'cb_person_default_on_file': 'historical_default'},inplace=True)\n",
        "\n",
        "# Rename the column from 'cb_person_cred_hist_length' to 'credit_history_length'\n",
        "df.rename(columns={'cb_person_cred_hist_length': 'credit_history_length'},inplace=True)"
      ],
      "metadata": {
        "id": "g_6W_oD-TGGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes to the names of the columns to be shorter and more understandable."
      ],
      "metadata": {
        "id": "i3BfB_VnSbUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether the column's name changed.\n",
        "df.info()"
      ],
      "metadata": {
        "id": "a5awmfZLTJM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making sure the changes to the names are saved into the dataframe."
      ],
      "metadata": {
        "id": "4BjvJpaASuD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the data in historical_default column to human-readable\n",
        "df['historical_default'] = df['historical_default'].replace({'Y': 'Yes', 'N': 'No'})"
      ],
      "metadata": {
        "id": "Oac-aJ7MTN37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes to the values inside the 'historical_default' column.\n",
        "\n",
        "Changes 'Y' into 'Yes' and 'N' into 'No'."
      ],
      "metadata": {
        "id": "UAFHDyVZS--6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the latest sense of the data.\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "SCL8Rz5BTQ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See what the dataframe currently looks like."
      ],
      "metadata": {
        "id": "9vQjJqrTTQwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation analysis\n",
        "df.corr()"
      ],
      "metadata": {
        "id": "IxFX7P05TTgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.corr() shows the strength and direction of the correlation between two columns in the dataframe.\n",
        "*   The number shows the strength of the correlation or the gradient if we were to graph the values in the two columns\n",
        "*   The  -  symbol shows that the gradient is negative if we were to graph the values in the two columns. The absence of the  -  symbol means the gradient is positive  "
      ],
      "metadata": {
        "id": "0rLHaN9kUVjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.3.2 Handling incomplete data/missing values"
      ],
      "metadata": {
        "id": "Uxv-Q3nPVTkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows that has even 1 missing value\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "z3SFqaV1TVv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.dropna() drops every rows that have missing or null values.\n",
        "\n",
        "inPlace make sure the changes is saved into the dataframe."
      ],
      "metadata": {
        "id": "ifY0y2yBVqH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether there are still have missing value\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "lj4HU_M8TXkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.isna().sum() shows missing or null values in the dataframe.\n",
        "\n",
        "No missing values are detected as they have been dropped in previous code."
      ],
      "metadata": {
        "id": "DdqQV2aRV8wK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.3.3 Remove outliers"
      ],
      "metadata": {
        "id": "d7gCoyDKWTch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use boxplot to find outliers.\n",
        "# Outliers are values that are higher or lower than most of the data.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a boxplot for the 'age' column\n",
        "ax = df.boxplot(column='age')\n",
        "\n",
        "# Set y-axis limits to cover the full range of the data\n",
        "ax.set_ylim(min(df['age']) - 10, max(df['age']) + 10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lDnAelLDTZ8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the values in age column in a boxplot to look for the outliers in the dataframe.\n",
        "\n",
        "Outliers are values that strayed to far away from the rest of the values."
      ],
      "metadata": {
        "id": "74ceNf0VWtui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with age over 70 because age limit for personal loan in Malaysia is 70\n",
        "overage = df['age'] > 70\n",
        "df.drop(df[overage].index, inplace = True)"
      ],
      "metadata": {
        "id": "v6dRWEoDTcbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the outliers and the age limit for personal loan in Malaysia, which is 70, we decided to drop rows that have age higher than 70."
      ],
      "metadata": {
        "id": "ZTQa7c3EXOUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.3.4 Remove nonsensible values"
      ],
      "metadata": {
        "id": "9I9KJa6kX8F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "JUbocG9JTe8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "df.describe() return the description of the data in the new dataframe after changes."
      ],
      "metadata": {
        "id": "53r7JNFIXe2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether there are not logic data\n",
        "x = df[df['employment_years'] > df['age']]\n",
        "x.head()"
      ],
      "metadata": {
        "id": "vHLFfW2HTg5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x is a new placeholder dataframe that is used to check for nonsensical values, which in this case is if someone have higher employment years than their age.\n",
        "\n",
        "x.head() shows the first few data containing the nonsensible values."
      ],
      "metadata": {
        "id": "XOkyBK4kYT6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows that employment_years is more than age\n",
        "result = df['employment_years'] > df['age']\n",
        "df.drop(df[result].index, inplace = True )"
      ],
      "metadata": {
        "id": "OW9V8fxWTjT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nonsensible data is saved into variable named result.\n",
        "\n",
        "The rows containing the nonsensible data is dropped and the changes is saved."
      ],
      "metadata": {
        "id": "oGCUS-AWZl77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether there still have not logic data\n",
        "x = df[df['employment_years'] > df['age']]\n",
        "x.head()"
      ],
      "metadata": {
        "id": "jclTQQXOTlH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x is used to check whether the nonsensible data remains in df dataframe.\n",
        "\n",
        "As they have been dropped, x returns nothing."
      ],
      "metadata": {
        "id": "5uJQCzofbvux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.3.5 Changing each columns' data type into more appropriate data types"
      ],
      "metadata": {
        "id": "Y4BXVz-IcMZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert employment_years from float64 to int64 since the data have no decimal point.\n",
        "\n",
        "df['employment_years'] = df['employment_years'].astype(np.int64)"
      ],
      "metadata": {
        "id": "381WksQzTnWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as the employment_years column have no decimal point, it is not necessary to use float64 as the data type as it takes unnecessary space. The datatype of all values in the column is changed into int64."
      ],
      "metadata": {
        "id": "E8hd7RprcehQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether there is no decimals in employment_years coloumn.\n",
        "df.head()\n",
        "# It looks more neatly now without '.0' ."
      ],
      "metadata": {
        "id": "WpxESSXiTpBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making sure the changes is saved into the dataframe."
      ],
      "metadata": {
        "id": "QP0bvB6ncyRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether data types for employment_years changed from float64 to int64.\n",
        "df.info()"
      ],
      "metadata": {
        "id": "OvsAxaA6TrX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making sure the changes is saved into the dataframe."
      ],
      "metadata": {
        "id": "vPj-O5jjc6-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is now cleaned"
      ],
      "metadata": {
        "id": "BaocQn3gLM9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Data Selection and Filtering"
      ],
      "metadata": {
        "id": "UfgJ1_90LPZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using all the features in the dataset, so we don't need specific code for data selection and filtering.\n",
        "\n",
        "We will directly proceed with data transformation."
      ],
      "metadata": {
        "id": "B7ewGEtCLSJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Data Transformation"
      ],
      "metadata": {
        "id": "WYrmxV53LT8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.5.1 Min-max normalization"
      ],
      "metadata": {
        "id": "HB8aBOyyLW_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before min-max normalization\n",
        "\n",
        "# Display histograms for each numerical feature in the DataFrame 'df'\n",
        "# Exclude 'loan_status' column because that is our target variable(y).\n",
        "# figsize=(15, 10) sets the size of the resulting figure to 15 inches in width and 10 inches in height.\n",
        "df.drop('loan_status', axis=1).hist(figsize=(15, 10))"
      ],
      "metadata": {
        "id": "q5lPhvnATuXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The purpose of creating histograms before normalization is to understand the distribution of values in each numerical feature.\n",
        "\n",
        "* Histograms provide insights into the spread and concentration of values, potential outliers and the general shape of the data distribution."
      ],
      "metadata": {
        "id": "Rrycbcqqeq9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Create a MinMaxScaler with a specified feature range of (0, 1)\n",
        "m=preprocessing.MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "# Create a copy of the original DataFrame 'df' for normalization\n",
        "df_norm_min_max=df.copy()\n",
        "\n",
        "# Apply min-max normalization to selected columns\n",
        "df_norm_min_max[['age','income','employment_years','loan_amount','loan_int_rate','loan_per_income','credit_history_length']]=m.fit_transform(df_norm_min_max[['age','income','employment_years','loan_amount','loan_int_rate','loan_per_income','credit_history_length']])\n",
        "\n",
        "# Display the first few rows of the DataFrame after min-max normalization\n",
        "df_norm_min_max.head()"
      ],
      "metadata": {
        "id": "_71xzVdJTxJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After this operation, the selected numerical features in df_norm_min_max will have values scaled to the range [0, 1].\n",
        "\n",
        "* This normalization will be used for machine learning algorithms that are sensitive to the scale of input features."
      ],
      "metadata": {
        "id": "1FNHcFE8gAYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After min-max normalization\n",
        "\n",
        "# Display histograms for each numerical feature in the DataFrame 'df'\n",
        "# figsize=(15, 10) sets the size of the resulting figure to 15 inches in width and 10 inches in height.\n",
        "df_norm_min_max.hist(figsize=(15,10))"
      ],
      "metadata": {
        "id": "I7k83ZtXTzUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The purpose of displaying histograms after min-max normalization is to observe the distribution of values in each numerical feature and to check whether the normalization has effectively scaled the values within the desired range.\n",
        "\n",
        "* This visualization helps in assessing the impact of normalization on the dataset and identifying any potential issues or improvements needed in the preprocessing steps."
      ],
      "metadata": {
        "id": "ACh2i60PhFDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.5.2 Z-score normalization"
      ],
      "metadata": {
        "id": "_XVgZpkRLgmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StandardScaler\n",
        "m=preprocessing.StandardScaler()\n",
        "\n",
        "# Create a copy of the original DataFrame 'df' for normalization\n",
        "df_norm_std_scaler=df.copy()\n",
        "\n",
        "# Apply standardization to selected columns\n",
        "df_norm_std_scaler[['age','income','employment_years','loan_amount','loan_int_rate','loan_per_income','credit_history_length']]=m.fit_transform(df_norm_std_scaler[['age','income','employment_years','loan_amount','loan_int_rate','loan_per_income','credit_history_length']])\n",
        "\n",
        "# Display histograms for each numerical feature in the DataFrame after standardization\n",
        "df_norm_std_scaler.hist(figsize=(15,10))"
      ],
      "metadata": {
        "id": "DhfDEx3gT1an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After this operation, the selected numerical features in df_norm_std_scaler will have a mean of 0 and a standard deviation of 1, which is a common requirement for machine learning algorithms that assume standardized input features.\n",
        "\n",
        "* The histograms help visualize the impact of standardization on the distribution of values in each feature."
      ],
      "metadata": {
        "id": "eT3cFDBaiK9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Data Aggregation"
      ],
      "metadata": {
        "id": "i9qC_ScZLj9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.6.1 Group the data by 'age'"
      ],
      "metadata": {
        "id": "-SLzuThfq8FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'age' and calculate the average income of the person.\n",
        "df.groupby('age')['income'].describe()"
      ],
      "metadata": {
        "id": "qbossGiQT4AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The describe() method calculates various summary statistics for each group, including count, mean, std (standard deviation), min, 25%, 50% (median), 75% and max.\n",
        "\n",
        "* This provides insights into the distribution of income values for each age group in the dataset."
      ],
      "metadata": {
        "id": "OiRVRfE8kkLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.6.2 Group the data by 'home_ownership'"
      ],
      "metadata": {
        "id": "Bt3hQmsmrgdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'home_ownership' and calculate the average income of the person.\n",
        "df.groupby('home_ownership')['income'].describe()"
      ],
      "metadata": {
        "id": "DtO6nd3nT6lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The describe() method calculates various summary statistics for each group, including count, mean, std (standard deviation), min, 25%, 50% (median), 75% and max.\n",
        "\n",
        "* This provides insights into the distribution of income values for each home_ownership group in the dataset."
      ],
      "metadata": {
        "id": "ban1UXSTk5NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.6.3 Group the data by 'loan_intent' and 'loan_status'"
      ],
      "metadata": {
        "id": "Wmfgwvq8sNcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'loan_intent' and 'loan_status' to calculate the average loan amount.\n",
        "df.groupby(['loan_intent','loan_status'])['loan_amount'].describe()"
      ],
      "metadata": {
        "id": "Pbxi1ILiT85x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The describe() method calculates various summary statistics for each group, including count, mean, std (standard deviation), min, 25%, 50% (median), 75% and max.\n",
        "\n",
        "* This provides insights into the distribution of loan amounts for each combination of loan intent and loan status in the dataset."
      ],
      "metadata": {
        "id": "Hf5u_CfHlJ18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.6.4 Group the data by 'loan_intent'"
      ],
      "metadata": {
        "id": "b7Jel6d2rsLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by loan_intent. Diffrent intent have diffrent loan amount range. (Medical vs Homeimprovement)\n",
        "\n",
        "# Display unique values in the 'loan_intent' column\n",
        "df['loan_intent'].unique()\n",
        "\n",
        "# Initialize a counter variable\n",
        "i=1\n",
        "\n",
        "# Iterate over unique loan intents and print them with corresponding indices\n",
        "for c in df['loan_intent'].unique():\n",
        "  print (f'{i}:{c}')\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "oZwwOMssUFOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The purpose of this code is to display the unique loan intents present in the 'loan_intent' column of the DataFrame along with their corresponding indices.\n",
        "\n",
        "* This information is useful for understanding the different categories or classes in the 'loan_intent' column and their distribution in the dataset."
      ],
      "metadata": {
        "id": "lj7xtZCWpLfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'loan_intent' and calculate the mean for each group\n",
        "df_by_intent=df.groupby('loan_intent').aggregate('mean')"
      ],
      "metadata": {
        "id": "r_lwxFvQUGxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first few rows of the grouped/aggregated data\n",
        "df_by_intent.head()"
      ],
      "metadata": {
        "id": "fVKBgoJSUITS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute additional summary statistics for the grouped/aggregated data\n",
        "df_by_intent.describe()"
      ],
      "metadata": {
        "id": "qtyDR6NLnFWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a bar plot with 'loan amount' as y-axis, with width 15 inches and height 20 inches\n",
        "df_by_intent.plot.bar(y='loan_amount',figsize=(15,20),use_index=True)"
      ],
      "metadata": {
        "id": "e_lTscyZUK0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* These steps helps us to explore and understand the characteristics of the data after performing groupby or aggregation operations."
      ],
      "metadata": {
        "id": "D2n9DWwNoVzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Data Visualization"
      ],
      "metadata": {
        "id": "ZxDRuLhKLuWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display descriptive statistics for each numerical column in the DataFrame 'df' before visualize the data\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "fyFrjmBXUM8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar plot with 'loan amount' column\n",
        "df['loan_amount'].plot.bar()"
      ],
      "metadata": {
        "id": "7mjstsUAUOqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The resulting bar plot have bars corresponding to each row or entry in the DataFrame, showcasing the 'loan_amount' values.\n",
        "\n",
        "* Bar plots are commonly used to visualize the distribution or variation of a numerical variable across different categories or data points.\n",
        "\n",
        "* But we have a large dataset, a bar plot of every individual data point is overwhelming and not informative.\n",
        "\n",
        "* This had proven that we should use scatter matrix as below."
      ],
      "metadata": {
        "id": "TqRO7WjsuDem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gets all the columns with numerical values\n",
        "df_numerical=df_norm_min_max._get_numeric_data()\n",
        "\n",
        "# Create a scatter matrix for the numerical columns\n",
        "pd.plotting.scatter_matrix(df_numerical,alpha=0.2,figsize=(15,15))"
      ],
      "metadata": {
        "id": "T5sBKzfYVc18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The scatter matrix provides a visual overview of the relationships between pairs of numerical variables in the dataset.\n",
        "\n",
        "* Each cell in the matrix represents a scatter plot of two variables and the diagonal cells show the distribution of individual variables.\n",
        "\n",
        "* This visualization is useful for identifying patterns, correlations and potential outliers in the data."
      ],
      "metadata": {
        "id": "P4M5krzrvU2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boxplot for each numerical column in df_numerical\n",
        "df_numerical.boxplot(figsize=(12,5))"
      ],
      "metadata": {
        "id": "hdAanlbBVmKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Each box represents the interquartile range (IQR) of the data with a line inside the box indicating the median.\n",
        "\n",
        "* The \"whiskers\" extend to the minimum and maximum values within a certain range and any points beyond the whiskers are considered potential outliers.\n",
        "\n",
        "* This visualization is helpful for identifying the central tendency, spread and presence of outliers in each numerical variable of the dataset.\n",
        "\n",
        "* It provides a quick and effective way to understand the distribution of the data across different features."
      ],
      "metadata": {
        "id": "3mNUldB3vl-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Takes a list of the dataframe columns\n",
        "cols=df.columns\n",
        "\n",
        "# Filter out categorical columns form the list of cols\n",
        "categorical=list(set(cols) - set(df_numerical))\n",
        "\n",
        "# Creates a dataframe only with categorical columns\n",
        "df_categorical = df[categorical]\n",
        "\n",
        "for col in df_categorical.columns:\n",
        "    # Counting the unique values in a column\n",
        "    n_unique = df_categorical[col].nunique()\n",
        "\n",
        "    # Execute statement below only if the column has <= 10 unique values\n",
        "    if(n_unique<=10):\n",
        "      # Count/categorize the number of samples in a unique value and plot it as a bar\n",
        "      df_categorical[col].value_counts().plot(kind='bar',figsize=(5,3))\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "t0rHTXxaVotG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This code is particularly useful for visualizing the distribution of categorical variables with a small number of unique values, allowing us to understand the frequency of each category in the dataset."
      ],
      "metadata": {
        "id": "vOaENGugwRUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dataset is now done with data pre-processing"
      ],
      "metadata": {
        "id": "UqlLN414L3i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0 Model Selection"
      ],
      "metadata": {
        "id": "Yq75XUsoVrVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.1 Set up the environement"
      ],
      "metadata": {
        "id": "6bOnRX3Py19U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install feature_engine\n",
        "!pip install catboost\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from plotly.offline import init_notebook_mode\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings, gc\n",
        "\n",
        "# Suppresses warning messages to avoid cluttering the output.\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Imports specific modules and classes from Scikit-Learn for preprocessing and model evaluation.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, PrecisionRecallDisplay, RocCurveDisplay\n",
        "\n",
        "# Imports statistical functions (probplot), outlier handling using Winsorizer from feature_engine and feature selection techniques such as dropping constant, correlated and duplicate features.\n",
        "from scipy.stats import probplot\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.selection import DropConstantFeatures, DropCorrelatedFeatures, DropDuplicateFeatures\n",
        "\n",
        "# Imports various machine learning models from XGBoost, CatBoost and LightGBM as well as a pipeline from Scikit-Learn.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier, XGBRFClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Imports the BorderlineSMOTE algorithm for handling imbalanced datasets from the imbalanced-learn library.\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "\n",
        "# Imports the Counter class from the collections module, which can be used for counting elements in a collection (useful for checking class distribution in imbalanced datasets).\n",
        "from collections import Counter\n",
        "\n",
        "# Imports the ClassPredictionError class from the Yellowbrick library, which is used for visualizing classification errors\n",
        "from yellowbrick.classifier import ClassPredictionError"
      ],
      "metadata": {
        "id": "DeYWR7RBXZ5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * This code sets up the environment by installing necessary libraries and importing a wide range of libraries and modules for machine learning."
      ],
      "metadata": {
        "id": "R4XOXfodyHR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.2 Early sense of the latest dataset before splitting the features to performs train-test split"
      ],
      "metadata": {
        "id": "mLEKbEI7y9v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of df_numerical to credit_risk\n",
        "# Model need only numerical data\n",
        "credit_risk = df_numerical.copy()"
      ],
      "metadata": {
        "id": "wr9PuoB4YjqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the 'credit_risk' DataFrame\n",
        "credit_risk.head()"
      ],
      "metadata": {
        "id": "zYHd485ZYz1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dimensions (number of rows and columns) of the 'credit_risk' DataFrame\n",
        "credit_risk.shape"
      ],
      "metadata": {
        "id": "wJdPeXXeZKfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the null value in each column\n",
        "credit_risk.isnull().sum()"
      ],
      "metadata": {
        "id": "_KEHMC_vZQn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and display the count of each unique value in the 'loan_status' column\n",
        "credit_risk['loan_status'].value_counts()"
      ],
      "metadata": {
        "id": "XSjpzxaoi90I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.3 Splitting features (employment_years, loan_amount, loan_int_rate, loan_status, loan_per_income, credit_history_length) and loan_status column"
      ],
      "metadata": {
        "id": "MCxnlVo9j712"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features (X) and target variable (y) for machine learning\n",
        "X = credit_risk.drop(['loan_status',\"age\",\"income\"], axis = 1)\n",
        "y = credit_risk['loan_status']"
      ],
      "metadata": {
        "id": "6RhDO9fCjdDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each step in the pipeline performs a specific operation:\n",
        "# 'constant': Drops constant features using the 'DropConstantFeatures' transformer.\n",
        "# 'correlated': Drops correlated features using the 'DropCorrelatedFeatures' transformer.\n",
        "# 'duplicate': Drops duplicate features using the 'DropDuplicateFeatures' transformer.\n",
        "\n",
        "# The 'pipeline' variable represents the entire sequence of operations.\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('constant',DropConstantFeatures()),\n",
        "    ('correlated',DropCorrelatedFeatures()),                              #function of pipeline\n",
        "    ('duplicate',DropDuplicateFeatures())\n",
        "])\n",
        "# Apply the pipeline to transform the features in X\n",
        "X = pipeline.fit_transform(X)\n",
        "# Display the dimensions of the transformed feature matrix\n",
        "X.shape"
      ],
      "metadata": {
        "id": "_tm-FnDADTux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BorderlineSMOTE is a variant of SMOTE that generates synthetic samples for the minority class while focusing on borderline instances\n",
        "smote = BorderlineSMOTE()\n",
        "# Apply SMOTE to resample the features (X) and target variable (y)\n",
        "X, y = smote.fit_resample(X, y)\n",
        "# Display the final dimensions of target label classes after applying SMOTE\n",
        "print(\"Final dimensions of target label classes:\", Counter(y))"
      ],
      "metadata": {
        "id": "jYXHNFFfEHFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2.4 Performs train-test split and scales the features using StandardScaler."
      ],
      "metadata": {
        "id": "YPou1Yd-6pRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into training and testing sets, ensuring that the class distribution is preserved (stratified split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42,stratify=y)\n",
        "#scaling variables\n",
        "scaler = StandardScaler()\n",
        "#scaler= RobustScaler()\n",
        "scaled_train_X = scaler.fit_transform(X_train)\n",
        "# Scale the testing features using the same scaler instance\n",
        "scaled_test_X = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "q1h95OO0Epa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.0 Model Training and Evaluation"
      ],
      "metadata": {
        "id": "DY3y_C0Mvsed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined function train_and_evaluate_model to train and evaluate a classification model\n",
        "\n",
        " function that trains and evaluates a classification model, displays various evaluation metrics, and stores performance metrics in lists for later analysis. The function is then called with a CatBoostClassifier. Adjust the code and comments based on your specific requirements and preferences."
      ],
      "metadata": {
        "id": "qz3vIw6Fvf4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 CatBoostClassifier Model"
      ],
      "metadata": {
        "id": "dX2jc2RfTZb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Lists to store model performance metrics\n",
        "models = []\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "roc_auc_scores = []\n",
        "\n",
        " # Fit the model on the training data\n",
        " # Make predictions on the testing data\n",
        " # Print the classification report\n",
        "def train_and_evaluate_model(model):\n",
        "    model.fit(scaled_train_X,y_train)\n",
        "    y_pred = model.predict(scaled_test_X)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test,y_pred))\n",
        "    print('-'*50)\n",
        "\n",
        "    # Display confusion matrix\n",
        "    ConfusionMatrixDisplay.from_predictions(y_test,y_pred)\n",
        "    # Display precision-recall curve\n",
        "    PrecisionRecallDisplay.from_predictions(y_test,y_pred)\n",
        "    # Display ROC curve\n",
        "    RocCurveDisplay.from_predictions(y_test,y_pred)\n",
        "\n",
        "    # Calculate and store performance metrics(1)\n",
        "    acc = accuracy_score(y_test,y_pred)\n",
        "    precision = precision_score(y_test,y_pred,average='macro')\n",
        "    recall = recall_score(y_test,y_pred,average='macro')\n",
        "    f1 = f1_score(y_test,y_pred,average='macro')\n",
        "    roc_auc = roc_auc_score(y_test,y_pred,average='macro')\n",
        "\n",
        "    # Additional visualization for non-CatBoost models\n",
        "    if re.search('catboost',str(model)) == None:\n",
        "        visualizer = ClassPredictionError(model)\n",
        "        visualizer.score(scaled_test_X,y_test)\n",
        "        visualizer.show()\n",
        "        del visualizer\n",
        "\n",
        "    # Calculate and store performance metrics(2)\n",
        "    accuracy_scores.append(acc)\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "    roc_auc_scores.append(roc_auc)\n",
        "    models.append(model)\n",
        "\n",
        "    # Cleanup and release memory\n",
        "    del acc, precision, recall, f1, roc_auc\n",
        "    gc.collect()\n",
        "\n",
        "    # Call the function with a CatBoostClassifier\n",
        "train_and_evaluate_model(CatBoostClassifier(silent=True))"
      ],
      "metadata": {
        "id": "vJ2Nx8UZE97A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 LGBMClassifier Model"
      ],
      "metadata": {
        "id": "RXefLz-5TdOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with a LGBMClassifier\n",
        "train_and_evaluate_model(LGBMClassifier(verbose=-1))"
      ],
      "metadata": {
        "id": "btqYq2q-HMg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 RandomForestClassifier Model"
      ],
      "metadata": {
        "id": "xwAfgfZIThw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Call the function with a RandomForestClassifier\n",
        "train_and_evaluate_model(RandomForestClassifier())"
      ],
      "metadata": {
        "id": "YqgAejSNHWvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 XGBClassifier Model"
      ],
      "metadata": {
        "id": "PjXMMy4NTnZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with an XGBClassifier\n",
        "train_and_evaluate_model(XGBClassifier())"
      ],
      "metadata": {
        "id": "5D2S5dWiHcc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 StackingClassifier Model"
      ],
      "metadata": {
        "id": "2U--LdSyTvKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier, StackingClassifier\n",
        "\n",
        "# Create a StackingClassifier with ExtraTrees, XGBoost, and CatBoost as base estimators, and RandomForest as the final estimator\n",
        "train_and_evaluate_model(StackingClassifier(estimators=[\n",
        "    ('ET',ExtraTreesClassifier()),\n",
        "    ('XGB',XGBClassifier()),\n",
        "    ('CAT',CatBoostClassifier(silent=True))\n",
        "],final_estimator=RandomForestClassifier(),verbose=2))"
      ],
      "metadata": {
        "id": "5nYq4GKmHgzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.0 Report On All Evaluation Metrics Between Models"
      ],
      "metadata": {
        "id": "MWHbsax9UACw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store model performance metrics\n",
        "model_perfs = pd.DataFrame({'Model': models,\n",
        "                            'Accuracy': accuracy_scores,\n",
        "                            'Precision': precision_scores,\n",
        "                            'Recall': recall_scores,\n",
        "                            'F1': f1_scores,\n",
        "                            'ROC-AUC': roc_auc_scores}).sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Check if arrays have the same length\n",
        "if len(roc_auc_scores) != len(models):\n",
        "    raise ValueError(\"All arrays must be of the same length\")\n",
        "\n",
        "    # Display the DataFrame\n",
        "model_perfs"
      ],
      "metadata": {
        "id": "7Ia1U1HBH5wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_importance(model, features, num=len(X), save=False):\n",
        "\n",
        "\n",
        "    #Plot feature importances using a bar plot.\n",
        "\n",
        "    #Parameters:\n",
        "    # - model: The trained machine learning model.\n",
        "    # - features: The DataFrame containing the features.\n",
        "    # - num: The number of top features to display (default is all features).\n",
        "    # - save: Whether to save the plot as an image file (default is False).\n",
        "\n",
        "    feature_imp = pd.DataFrame({\"Value\": model.feature_importances_, \"Feature\": features.columns})\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.set(font_scale=1)\n",
        "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:num])\n",
        "    plt.title(\"Features\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    if save:\n",
        "        plt.savefig(\"importances.png\")\n",
        "\n",
        "# Example usage with CatBoostClassifier\n",
        "model = CatBoostClassifier(silent=True)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Assuming X is a DataFrame containing the features\n",
        "plot_importance(model, X)"
      ],
      "metadata": {
        "id": "iAC6C-_mJZSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Example usage with RandomForestClassifier\n",
        "X = credit_risk.drop(['loan_status',\"age\",\"income\"], axis = 1)\n",
        "y = credit_risk['loan_status']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Assuming X is a DataFrame containing the features\n",
        "plot_importance(rf_model,X)\n",
        "\n",
        "# Example usage with XGBClassifier\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "plot_importance(xgb_model,X)\n",
        "\n",
        "# Example usage with LGBMClassifier\n",
        "lgbm_model = LGBMClassifier()\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "plot_importance(lgbm_model,X)\n",
        "\n",
        "# Example usage with DecisionTreeClassifier\n",
        "dt_model = DecisionTreeClassifier()\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "plot_importance(dt_model,X)\n",
        "\n",
        "# Create a StackingClassifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('ET', ExtraTreesClassifier()),\n",
        "        ('XGB', XGBClassifier()),\n",
        "        ('CAT', CatBoostClassifier(silent=True))\n",
        "    ],\n",
        "    final_estimator=RandomForestClassifier(),\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit the StackingClassifier on the training data\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Access feature importances from each base estimator\n",
        "feature_importances = {}\n",
        "\n",
        "for name, estimator in stacking_model.named_estimators_.items():\n",
        "    if hasattr(estimator, 'feature_importances_'):\n",
        "        feature_importances[name] = estimator.feature_importances_\n",
        "\n",
        "# Plot or display feature importances for each base estimator\n",
        "for name, importances in feature_importances.items():\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(range(len(importances)), importances, tick_label=X_train.columns)\n",
        "    plt.title(f\"Feature Importances - {name}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "42zd2OvuVVx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.dtypes)"
      ],
      "metadata": {
        "id": "JwZDkCmmdCXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.0 Summarise of experiments with models and dataset"
      ],
      "metadata": {
        "id": "tnsh5ABrlfYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = credit_risk.drop(['loan_status', 'age', 'income'], axis=1)\n",
        "y = credit_risk['loan_status']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "results_table = []\n",
        "\n",
        "# Experiment 1: Data Transformation using Min-Max Scaler\n",
        "# Apply Min-Max scaling to X_train\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
        "X_test_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "# Random Forest model with Min-Max scaling\n",
        "rf_model_minmax = RandomForestClassifier()\n",
        "rf_model_minmax.fit(X_train_minmax, y_train)\n",
        "y_pred_minmax_rf = rf_model_minmax.predict(X_test_minmax)\n",
        "accuracy_minmax_rf = accuracy_score(y_test, y_pred_minmax_rf)\n",
        "results_table.append([\"Experiment 1\", \"Random Forest with Min-Max Scaler\", accuracy_minmax_rf])\n",
        "\n",
        "# XGBoost model with Min-Max scaling\n",
        "xgb_model_minmax = XGBClassifier()\n",
        "xgb_model_minmax.fit(X_train_minmax, y_train)\n",
        "y_pred_minmax_xgb = xgb_model_minmax.predict(X_test_minmax)\n",
        "accuracy_minmax_xgb = accuracy_score(y_test, y_pred_minmax_xgb)\n",
        "results_table.append([\"Experiment 1\", \"XGBoost with Min-Max Scaler\", accuracy_minmax_xgb])\n",
        "\n",
        "# CatBoost with Min-Max scaling\n",
        "catboost_model_minmax = CatBoostClassifier(silent=True)\n",
        "catboost_model_minmax.fit(X_train_minmax, y_train)\n",
        "y_pred_minmax_catboost = catboost_model_minmax.predict(X_test_minmax)\n",
        "accuracy_minmax_catboost = accuracy_score(y_test, y_pred_minmax_catboost)\n",
        "results_table.append([\"Experiment 1\", \"CatBoost with Min-Max Scaler\", accuracy_minmax_catboost])\n",
        "\n",
        "# LightGBM with Min-Max scaling\n",
        "lgbm_model_minmax = LGBMClassifier()\n",
        "lgbm_model_minmax.fit(X_train_minmax, y_train)\n",
        "y_pred_minmax_lgbm = lgbm_model_minmax.predict(X_test_minmax)\n",
        "accuracy_minmax_lgbm = accuracy_score(y_test, y_pred_minmax_lgbm)\n",
        "results_table.append([\"Experiment 1\", \"LightGBM with Min-Max Scaler\", accuracy_minmax_lgbm])\n",
        "\n",
        "# Stacking with Min-Max scaling\n",
        "estimators_minmax = [\n",
        "    ('CatBoost', CatBoostClassifier(silent=True)),\n",
        "    ('LGBM', LGBMClassifier()),\n",
        "    ('DecisionTree', DecisionTreeClassifier())\n",
        "]\n",
        "\n",
        "# Implementing the Machine Learning model using a stacking ensemble approach with Min-Max scaling\n",
        "stacking_model_minmax = StackingClassifier(estimators=estimators_minmax, final_estimator=DecisionTreeClassifier())\n",
        "stacking_model_minmax.fit(X_train_minmax, y_train)\n",
        "y_pred_minmax_stacking = stacking_model_minmax.predict(X_test_minmax)\n",
        "accuracy_minmax_stacking = accuracy_score(y_test, y_pred_minmax_stacking)\n",
        "results_table.append([\"Experiment 1\", \"Stacking with Min-Max Scaler\", accuracy_minmax_stacking])\n",
        "\n",
        "# Experiment 2: Data Transformation using Standard Scaler\n",
        "# Apply Standard scaling to X_train\n",
        "scaler_standard = StandardScaler()\n",
        "X_train_standard = scaler_standard.fit_transform(X_train)\n",
        "X_test_standard = scaler_standard.transform(X_test)\n",
        "\n",
        "# Random Forest model with Standard scaling\n",
        "rf_model_standard = RandomForestClassifier()\n",
        "rf_model_standard.fit(X_train_standard, y_train)\n",
        "y_pred_standard_rf = rf_model_standard.predict(X_test_standard)\n",
        "accuracy_standard_rf = accuracy_score(y_test, y_pred_standard_rf)\n",
        "results_table.append([\"Experiment 2\", \"Random Forest with Standard Scaler\", accuracy_standard_rf])\n",
        "\n",
        "# XGBoost model with Standard scaling\n",
        "xgb_model_standard = XGBClassifier()\n",
        "xgb_model_standard.fit(X_train_standard, y_train)\n",
        "y_pred_standard_xgb = xgb_model_standard.predict(X_test_standard)\n",
        "accuracy_standard_xgb = accuracy_score(y_test, y_pred_standard_xgb)\n",
        "results_table.append([\"Experiment 2\", \"XGBoost with Standard Scaler\", accuracy_standard_xgb])\n",
        "\n",
        "# CatBoost with Standard scaling\n",
        "catboost_model_standard = CatBoostClassifier(silent=True)\n",
        "catboost_model_standard.fit(X_train_standard, y_train)\n",
        "y_pred_standard_catboost = catboost_model_standard.predict(X_test_standard)\n",
        "accuracy_standard_catboost = accuracy_score(y_test, y_pred_standard_catboost)\n",
        "results_table.append([\"Experiment 2\", \"CatBoost with Standard Scaler\", accuracy_standard_catboost])\n",
        "\n",
        "# LightGBM with Standard scaling\n",
        "lgbm_model_standard = LGBMClassifier()\n",
        "lgbm_model_standard.fit(X_train_standard, y_train)\n",
        "y_pred_standard_lgbm = lgbm_model_standard.predict(X_test_standard)\n",
        "accuracy_standard_lgbm = accuracy_score(y_test, y_pred_standard_lgbm)\n",
        "results_table.append([\"Experiment 2\", \"LightGBM with Standard Scaler\", accuracy_standard_lgbm])\n",
        "\n",
        "# Stacking with Standard scaling\n",
        "estimators_standard = [\n",
        "    ('CatBoost', CatBoostClassifier(silent=True)),\n",
        "    ('LGBM', LGBMClassifier()),\n",
        "    ('DecisionTree', DecisionTreeClassifier())\n",
        "]\n",
        "\n",
        "#Implementing the Machine Learning model using a stacking ensemble approach with Standard scaling for data preprocessing\n",
        "stacking_model_standard = StackingClassifier(estimators=estimators_standard, final_estimator=DecisionTreeClassifier())\n",
        "stacking_model_standard.fit(X_train_standard, y_train)\n",
        "y_pred_standard_stacking = stacking_model_standard.predict(X_test_standard)\n",
        "accuracy_standard_stacking = accuracy_score(y_test, y_pred_standard_stacking)\n",
        "results_table.append([\"Experiment 2\", \"Stacking with Standard Scaler\", accuracy_standard_stacking])\n",
        "\n",
        "# Display results in table format\n",
        "headers = [\"Experiment\", \"Method\", \"Accuracy\"]\n",
        "print(tabulate(results_table, headers=headers, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "id": "0Vg58EmjYkpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.0 Conclusion\n",
        "The output shows the accuracy of different machine learning algorithms on the dataset under different experiments, considering two types of scalers: Min-Max Scaler and Standard Scaler.\n",
        "\n",
        "1) Random Forest, XGBoost, CatBoost, LightGBM:\n",
        "* Random Forest achieved the highest accuracy among all algorithms with both Min-Max and Standard Scalers.\n",
        "* XGBoost, CatBoost and LightGBM performed competitively, with similar accuracy values.\n",
        "\n",
        "2) Scalers Comparison:\n",
        "* There is no significant difference in performance between Min-Max Scaler and Standard Scaler for most algorithms.\n",
        "* In some cases, Standard Scaler slightly outperformed Min-Max Scaler and vice versa.\n",
        "\n",
        "3) Stacking:\n",
        "* The stacking approach, where multiple models are combined, performed slightly lower than individual algorithms in both Min-Max and Standard Scaler experiments.\n",
        "\n",
        "4) Overall:\n",
        "* Random Forest appears to be a strong performer on the dataset.\n",
        "* XGBoost, CatBoost and LightGBM, being gradient boosting algorithms, also provided good results.\n",
        "* The choice between Min-Max Scaler and Standard Scaler depend on the specific characteristics of the dataset and the performance difference is relatively small.\n",
        "\n",
        "* From the table above, proven that the best algorithm for this dataset is Random Forest with Min-Max Scaler with accuracy of 0.873734.\n",
        "\n",
        "* While the worst algorithm for this dataset is Stacking with Min-Max Scaler with accuracy of 0.794970."
      ],
      "metadata": {
        "id": "Fg-CifaB_E6n"
      }
    }
  ]
}